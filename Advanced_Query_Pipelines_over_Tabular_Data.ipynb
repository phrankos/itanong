{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "CDhaOQ9wKCir",
      "metadata": {
        "id": "CDhaOQ9wKCir"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9jzY0AXxCxxr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jzY0AXxCxxr",
        "outputId": "8113d026-ab37-4d89-ee5e-cd14182af6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.42-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl.metadata (678 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core==0.10.42 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.42-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.22-py3-none-any.whl.metadata (559 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.23-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting PyYAML>=6.0.1 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading SQLAlchemy-2.0.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting httpx (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting networkx>=3.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting numpy (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=1.1.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading openai-1.30.5-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow>=9.0.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting requests>=2.31.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m298.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting wrapt (from llama-index-core==0.10.42->llama-index)\n",
            "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio (from httpx->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting certifi (from httpx->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting idna (from httpx->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting sniffio (from httpx->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m314.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting packaging>=17.0 (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.18.3 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading pydantic_core-2.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.42->llama-index)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading llama_index-0.10.42-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_core-0.10.42-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
            "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.1.22-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.23-py3-none-any.whl (36 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.4.4-py3-none-any.whl (8.0 kB)\n",
            "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m201.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m276.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.30.5-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m421.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m296.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m166.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.0/620.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m195.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.1/328.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m372.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m386.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: striprtf, pytz, dirtyjson, wrapt, urllib3, tzdata, typing-extensions, tqdm, tenacity, soupsieve, sniffio, six, regex, PyYAML, pypdf, pillow, packaging, numpy, networkx, nest-asyncio, mypy-extensions, multidict, joblib, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, python-dateutil, pydantic-core, nltk, marshmallow, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, pydantic, pandas, httpx, dataclasses-json, aiohttp, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: striprtf\n",
            "    Found existing installation: striprtf 0.0.26\n",
            "    Uninstalling striprtf-0.0.26:\n",
            "      Successfully uninstalled striprtf-0.0.26\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.1\n",
            "    Uninstalling pytz-2024.1:\n",
            "      Successfully uninstalled pytz-2024.1\n",
            "  Attempting uninstall: dirtyjson\n",
            "    Found existing installation: dirtyjson 1.0.8\n",
            "    Uninstalling dirtyjson-1.0.8:\n",
            "      Successfully uninstalled dirtyjson-1.0.8\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.1\n",
            "    Uninstalling urllib3-2.2.1:\n",
            "      Successfully uninstalled urllib3-2.2.1\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2024.1\n",
            "    Uninstalling tzdata-2024.1:\n",
            "      Successfully uninstalled tzdata-2024.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.1\n",
            "    Uninstalling typing_extensions-4.12.1:\n",
            "      Successfully uninstalled typing_extensions-4.12.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.3.0\n",
            "    Uninstalling tenacity-8.3.0:\n",
            "      Successfully uninstalled tenacity-8.3.0\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.5\n",
            "    Uninstalling soupsieve-2.5:\n",
            "      Successfully uninstalled soupsieve-2.5\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.1\n",
            "    Uninstalling sniffio-1.3.1:\n",
            "      Successfully uninstalled sniffio-1.3.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.5.15\n",
            "    Uninstalling regex-2024.5.15:\n",
            "      Successfully uninstalled regex-2024.5.15\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: pypdf\n",
            "    Found existing installation: pypdf 4.2.0\n",
            "    Uninstalling pypdf-4.2.0:\n",
            "      Successfully uninstalled pypdf-4.2.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.3.0\n",
            "    Uninstalling pillow-10.3.0:\n",
            "      Successfully uninstalled pillow-10.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.6.0\n",
            "    Uninstalling nest-asyncio-1.6.0:\n",
            "      Successfully uninstalled nest-asyncio-1.6.0\n",
            "  Attempting uninstall: mypy-extensions\n",
            "    Found existing installation: mypy-extensions 1.0.0\n",
            "    Uninstalling mypy-extensions-1.0.0:\n",
            "      Successfully uninstalled mypy-extensions-1.0.0\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.0.5\n",
            "    Uninstalling multidict-6.0.5:\n",
            "      Successfully uninstalled multidict-6.0.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 3.0.3\n",
            "    Uninstalling greenlet-3.0.3:\n",
            "      Successfully uninstalled greenlet-3.0.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.5.0\n",
            "    Uninstalling fsspec-2024.5.0:\n",
            "      Successfully uninstalled fsspec-2024.5.0\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.4.1\n",
            "    Uninstalling frozenlist-1.4.1:\n",
            "      Successfully uninstalled frozenlist-1.4.1\n",
            "  Attempting uninstall: distro\n",
            "    Found existing installation: distro 1.9.0\n",
            "    Uninstalling distro-1.9.0:\n",
            "      Successfully uninstalled distro-1.9.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.6.2\n",
            "    Uninstalling certifi-2024.6.2:\n",
            "      Successfully uninstalled certifi-2024.6.2\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.9.4\n",
            "    Uninstalling yarl-1.9.4:\n",
            "      Successfully uninstalled yarl-1.9.4\n",
            "  Attempting uninstall: typing-inspect\n",
            "    Found existing installation: typing-inspect 0.9.0\n",
            "    Uninstalling typing-inspect-0.9.0:\n",
            "      Successfully uninstalled typing-inspect-0.9.0\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.30\n",
            "    Uninstalling SQLAlchemy-2.0.30:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.30\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.18.3\n",
            "    Uninstalling pydantic_core-2.18.3:\n",
            "      Successfully uninstalled pydantic_core-2.18.3\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: marshmallow\n",
            "    Found existing installation: marshmallow 3.21.2\n",
            "    Uninstalling marshmallow-3.21.2:\n",
            "      Successfully uninstalled marshmallow-3.21.2\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.5\n",
            "    Uninstalling httpcore-1.0.5:\n",
            "      Successfully uninstalled httpcore-1.0.5\n",
            "  Attempting uninstall: deprecated\n",
            "    Found existing installation: Deprecated 1.2.14\n",
            "    Uninstalling Deprecated-1.2.14:\n",
            "      Successfully uninstalled Deprecated-1.2.14\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.4.0\n",
            "    Uninstalling anyio-4.4.0:\n",
            "      Successfully uninstalled anyio-4.4.0\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.3.1\n",
            "    Uninstalling aiosignal-1.3.1:\n",
            "      Successfully uninstalled aiosignal-1.3.1\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.7.0\n",
            "    Uninstalling tiktoken-0.7.0:\n",
            "      Successfully uninstalled tiktoken-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.2\n",
            "    Uninstalling pydantic-2.7.2:\n",
            "      Successfully uninstalled pydantic-2.7.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.0\n",
            "    Uninstalling httpx-0.27.0:\n",
            "      Successfully uninstalled httpx-0.27.0\n",
            "  Attempting uninstall: dataclasses-json\n",
            "    Found existing installation: dataclasses-json 0.6.6\n",
            "    Uninstalling dataclasses-json-0.6.6:\n",
            "      Successfully uninstalled dataclasses-json-0.6.6\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.5\n",
            "    Uninstalling aiohttp-3.9.5:\n",
            "      Successfully uninstalled aiohttp-3.9.5\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.30.5\n",
            "    Uninstalling openai-1.30.5:\n",
            "      Successfully uninstalled openai-1.30.5\n",
            "  Attempting uninstall: llamaindex-py-client\n",
            "    Found existing installation: llamaindex-py-client 0.1.19\n",
            "    Uninstalling llamaindex-py-client-0.1.19:\n",
            "      Successfully uninstalled llamaindex-py-client-0.1.19\n",
            "  Attempting uninstall: llama-index-legacy\n",
            "    Found existing installation: llama-index-legacy 0.9.48\n",
            "    Uninstalling llama-index-legacy-0.9.48:\n",
            "      Successfully uninstalled llama-index-legacy-0.9.48\n",
            "  Attempting uninstall: llama-index-core\n",
            "    Found existing installation: llama-index-core 0.10.42\n",
            "    Uninstalling llama-index-core-0.10.42:\n",
            "      Successfully uninstalled llama-index-core-0.10.42\n",
            "  Attempting uninstall: llama-parse\n",
            "    Found existing installation: llama-parse 0.4.4\n",
            "    Uninstalling llama-parse-0.4.4:\n",
            "      Successfully uninstalled llama-parse-0.4.4\n",
            "  Attempting uninstall: llama-index-readers-file\n",
            "    Found existing installation: llama-index-readers-file 0.1.23\n",
            "    Uninstalling llama-index-readers-file-0.1.23:\n",
            "      Successfully uninstalled llama-index-readers-file-0.1.23\n",
            "  Attempting uninstall: llama-index-llms-openai\n",
            "    Found existing installation: llama-index-llms-openai 0.1.22\n",
            "    Uninstalling llama-index-llms-openai-0.1.22:\n",
            "      Successfully uninstalled llama-index-llms-openai-0.1.22\n",
            "  Attempting uninstall: llama-index-indices-managed-llama-cloud\n",
            "    Found existing installation: llama-index-indices-managed-llama-cloud 0.1.6\n",
            "    Uninstalling llama-index-indices-managed-llama-cloud-0.1.6:\n",
            "      Successfully uninstalled llama-index-indices-managed-llama-cloud-0.1.6\n",
            "  Attempting uninstall: llama-index-embeddings-openai\n",
            "    Found existing installation: llama-index-embeddings-openai 0.1.10\n",
            "    Uninstalling llama-index-embeddings-openai-0.1.10:\n",
            "      Successfully uninstalled llama-index-embeddings-openai-0.1.10\n",
            "  Attempting uninstall: llama-index-readers-llama-parse\n",
            "    Found existing installation: llama-index-readers-llama-parse 0.1.4\n",
            "    Uninstalling llama-index-readers-llama-parse-0.1.4:\n",
            "      Successfully uninstalled llama-index-readers-llama-parse-0.1.4\n",
            "  Attempting uninstall: llama-index-multi-modal-llms-openai\n",
            "    Found existing installation: llama-index-multi-modal-llms-openai 0.1.6\n",
            "    Uninstalling llama-index-multi-modal-llms-openai-0.1.6:\n",
            "      Successfully uninstalled llama-index-multi-modal-llms-openai-0.1.6\n",
            "  Attempting uninstall: llama-index-cli\n",
            "    Found existing installation: llama-index-cli 0.1.12\n",
            "    Uninstalling llama-index-cli-0.1.12:\n",
            "      Successfully uninstalled llama-index-cli-0.1.12\n",
            "  Attempting uninstall: llama-index-agent-openai\n",
            "    Found existing installation: llama-index-agent-openai 0.2.7\n",
            "    Uninstalling llama-index-agent-openai-0.2.7:\n",
            "      Successfully uninstalled llama-index-agent-openai-0.2.7\n",
            "  Attempting uninstall: llama-index-program-openai\n",
            "    Found existing installation: llama-index-program-openai 0.1.6\n",
            "    Uninstalling llama-index-program-openai-0.1.6:\n",
            "      Successfully uninstalled llama-index-program-openai-0.1.6\n",
            "  Attempting uninstall: llama-index-question-gen-openai\n",
            "    Found existing installation: llama-index-question-gen-openai 0.1.3\n",
            "    Uninstalling llama-index-question-gen-openai-0.1.3:\n",
            "      Successfully uninstalled llama-index-question-gen-openai-0.1.3\n",
            "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 attrs-23.2.0 beautifulsoup4-4.12.3 certifi-2024.6.2 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.6 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.5.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 joblib-1.4.2 llama-index-0.10.42 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.42 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.22 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.23 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.4 llamaindex-py-client-0.1.19 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 nest-asyncio-1.6.0 networkx-3.3 nltk-3.8.1 numpy-1.26.4 openai-1.30.5 packaging-24.0 pandas-2.2.2 pillow-10.3.0 pydantic-2.7.2 pydantic-core-2.18.3 pypdf-4.2.0 python-dateutil-2.9.0.post0 pytz-2024.1 regex-2024.5.15 requests-2.32.3 six-1.16.0 sniffio-1.3.1 soupsieve-2.5 striprtf-0.0.26 tenacity-8.3.0 tiktoken-0.7.0 tqdm-4.66.4 typing-extensions-4.12.1 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 wrapt-1.16.0 yarl-1.9.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index --upgrade --no-cache-dir --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44627d9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index-llms-huggingface\n",
        "%pip install \"transformers[torch]\" \"huggingface_hub[inference]\"\n",
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e16e7f46-6a3f-4390-b149-51d49a255d54",
      "metadata": {
        "id": "e16e7f46-6a3f-4390-b149-51d49a255d54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/theebatican/anaconda3/envs/itanong/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/theebatican/anaconda3/envs/itanong/lib/python3.11/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/theebatican/.cache/huggingface/token\n",
            "Login successful\n",
            "ninyx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]\n",
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:llama_index.llms.huggingface.base:The model `meta-llama/Meta-Llama-3-8B` and tokenizer `StabilityAI/stablelm-tuned-alpha-3b` are different, please ensure that they are compatible.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import List, Optional\n",
        "\n",
        "from llama_index.llms.huggingface import (\n",
        "    HuggingFaceInferenceAPI,\n",
        "    HuggingFaceLLM,\n",
        ")\n",
        "\n",
        "os.system(f'huggingface-cli login --token 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')\n",
        "os.system('huggingface-cli whoami')\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "# SEE: https://huggingface.co/docs/hub/security-tokens\n",
        "# We just need a token with read permissions for this demo\n",
        "# NOTE: None default will fall back on Hugging Face's token storage\n",
        "# when this token gets used within HuggingFaceInferenceAPI\n",
        "\n",
        "\n",
        "# This uses https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n",
        "# downloaded (if first invocation) to the local Hugging Face model cache,\n",
        "# and actually runs the model on your local machine's hardware\n",
        "locally_run = HuggingFaceLLM(model_name=model_name)\n",
        "\n",
        "from llama_index.core import set_global_tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "set_global_tokenizer(\n",
        "    AutoTokenizer.from_pretrained(model_name).encode\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e736b7cb",
      "metadata": {},
      "source": [
        "# Query Pipeline over Pandas DataFrames\n",
        "\n",
        "This is a simple example that builds a query pipeline that can perform structured operations over a Pandas DataFrame to satisfy a user query, using LLMs to infer the set of operations.\n",
        "\n",
        "This can be treated as the \"from-scratch\" version of our `PandasQueryEngine`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "844f1db7-8eaa-4fb2-91f4-a363db66bd5b",
      "metadata": {
        "id": "844f1db7-8eaa-4fb2-91f4-a363db66bd5b"
      },
      "source": [
        "## Download Data\n",
        "\n",
        "Here we load the Titanic CSV dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c3d6f4-ec01-4992-8dbf-696b6feeda5f",
      "metadata": {
        "id": "31c3d6f4-ec01-4992-8dbf-696b6feeda5f",
        "outputId": "d189f285-a045-4a1b-b0f8-d49f45125e95"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/csv/titanic_train.csv' -O 'titanic_train.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f94d45-607c-4f04-a05c-b66ecedfe233",
      "metadata": {
        "id": "b2f94d45-607c-4f04-a05c-b66ecedfe233"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./titanic_train.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42ccfe1-fd13-4356-b5b1-c9eabac5f391",
      "metadata": {
        "id": "b42ccfe1-fd13-4356-b5b1-c9eabac5f391"
      },
      "source": [
        "## Define Modules\n",
        "\n",
        "Here we define the set of modules:\n",
        "1. Pandas prompt to infer pandas instructions from user query\n",
        "2. Pandas output parser to execute pandas instructions on dataframe, get back dataframe\n",
        "3. Response synthesis prompt to synthesize a final response given the dataframe\n",
        "4. LLM\n",
        "\n",
        "The pandas output parser specifically is designed to safely execute Python code. It includes a lot of safety checks that may be annoying to write from scratch. This includes only importing from a set of approved modules (e.g. no modules that would alter the file system like `os`), and also making sure that no private/dunder methods are being called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47ac723-e1fc-47c2-9736-19deb50d87ae",
      "metadata": {
        "id": "d47ac723-e1fc-47c2-9736-19deb50d87ae"
      },
      "outputs": [],
      "source": [
        "instruction_str = (\n",
        "    \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
        "    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
        "    \"3. The code should represent a solution to the query.\\n\"\n",
        "    \"4. PRINT ONLY THE EXPRESSION.\\n\"\n",
        "    \"5. Do not quote the expression.\\n\"\n",
        ")\n",
        "\n",
        "pandas_prompt_str = (\n",
        "    \"You are working with a pandas dataframe in Python.\\n\"\n",
        "    \"The name of the dataframe is `df`.\\n\"\n",
        "    \"This is the result of `print(df.head())`:\\n\"\n",
        "    \"{df_str}\\n\\n\"\n",
        "    \"Follow these instructions:\\n\"\n",
        "    \"{instruction_str}\\n\"\n",
        "    \"Query: {query_str}\\n\\n\"\n",
        "    \"Expression:\"\n",
        ")\n",
        "response_synthesis_prompt_str = (\n",
        "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
        "    \"Query: {query_str}\\n\\n\"\n",
        "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
        "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
        "    \"Response: \"\n",
        ")\n",
        "\n",
        "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
        "    instruction_str=instruction_str, df_str=df.head(5)\n",
        ")\n",
        "pandas_output_parser = PandasInstructionParser(df)\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7907f6-2926-4c83-9541-d089d0b0af88",
      "metadata": {
        "id": "1b7907f6-2926-4c83-9541-d089d0b0af88"
      },
      "source": [
        "## Build Query Pipeline\n",
        "\n",
        "Looks like this:\n",
        "input query_str -> pandas_prompt -> llm1 -> pandas_output_parser -> response_synthesis_prompt -> llm2\n",
        "\n",
        "Additional connections to response_synthesis_prompt: llm1 -> pandas_instructions, and pandas_output_parser -> pandas_output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3544a6a8-3866-4806-8108-d908bd5d17e6",
      "metadata": {
        "id": "3544a6a8-3866-4806-8108-d908bd5d17e6"
      },
      "outputs": [],
      "source": [
        "qp = QP(\n",
        "    modules={\n",
        "        \"input\": InputComponent(),\n",
        "        \"pandas_prompt\": pandas_prompt,\n",
        "        \"llm1\": llm,\n",
        "        \"pandas_output_parser\": pandas_output_parser,\n",
        "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "        \"llm2\": llm,\n",
        "    },\n",
        "    verbose=True,\n",
        ")\n",
        "qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
        "qp.add_links(\n",
        "    [\n",
        "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
        "        Link(\n",
        "            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n",
        "        ),\n",
        "        Link(\n",
        "            \"pandas_output_parser\",\n",
        "            \"response_synthesis_prompt\",\n",
        "            dest_key=\"pandas_output\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "# add link from response synthesis prompt to llm2\n",
        "qp.add_link(\"response_synthesis_prompt\", \"llm2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1886a4-72c0-40e9-94e4-512feeb3da34",
      "metadata": {
        "id": "0c1886a4-72c0-40e9-94e4-512feeb3da34",
        "outputId": "9bc4d1cc-c109-4362-f4a0-2fb82782c179"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
        "net.from_nx(qp.dag)\n",
        "net.show(\"text2sql_dag.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea9cba7-733b-4087-a297-5c2799a8cfe5",
      "metadata": {
        "id": "dea9cba7-733b-4087-a297-5c2799a8cfe5"
      },
      "source": [
        "## Run Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49237e3f-45f8-4b9b-8e32-0d31555b3539",
      "metadata": {
        "id": "49237e3f-45f8-4b9b-8e32-0d31555b3539",
        "outputId": "0582c2e6-145f-42ce-c89b-36f222706272"
      },
      "outputs": [],
      "source": [
        "response = qp.run(\n",
        "    query_str=\"What is the correlation between survival and age?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17345c60-65ec-4053-b992-cc1f790f2b55",
      "metadata": {
        "id": "17345c60-65ec-4053-b992-cc1f790f2b55",
        "outputId": "ffd8660a-f655-4150-e585-55b6c621d6f0"
      },
      "outputs": [],
      "source": [
        "print(response.message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "855f9f50-ef38-4069-932a-fb49af02d28e",
      "metadata": {
        "id": "855f9f50-ef38-4069-932a-fb49af02d28e"
      },
      "source": [
        "# Query Pipeline for Advanced Text-to-SQL\n",
        "\n",
        "In this guide we show you how to setup a text-to-SQL pipeline over your data with our [query pipeline](https://docs.llamaindex.ai/en/stable/module_guides/querying/pipeline/root.html) syntax.\n",
        "\n",
        "This gives you flexibility to enhance text-to-SQL with additional techniques. We show these in the below sections:\n",
        "1. **Query-Time Table Retrieval**: Dynamically retrieve relevant tables in the text-to-SQL prompt.\n",
        "2. **Query-Time Sample Row retrieval**: Embed/Index each row, and dynamically retrieve example rows for each table in the text-to-SQL prompt.\n",
        "\n",
        "Our out-of-the box pipelines include our `NLSQLTableQueryEngine` and `SQLTableRetrieverQueryEngine`. (if you want to check out our text-to-SQL guide using these modules, take a look [here](https://docs.llamaindex.ai/en/stable/examples/index_structs/struct_indices/SQLIndexDemo.html)). This guide implements an advanced version of those modules, giving you the utmost flexibility to apply this to your own setting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39cc96c-9fbc-44c3-b57f-017a9aa75473",
      "metadata": {
        "id": "e39cc96c-9fbc-44c3-b57f-017a9aa75473"
      },
      "source": [
        "## Load and Ingest Data\n",
        "\n",
        "\n",
        "### Load Data\n",
        "We use the [WikiTableQuestions dataset](https://ppasupat.github.io/WikiTableQuestions/) (Pasupat and Liang 2015) as our test dataset.\n",
        "\n",
        "We go through all the csv's in one folder, store each in a sqlite database (we will then build an object index over each table schema)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7c869b-d273-4b5b-a2e7-7a7849619354",
      "metadata": {
        "id": "cc7c869b-d273-4b5b-a2e7-7a7849619354",
        "outputId": "cd5d7c90-51de-4765-ed37-3f668a42f945",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!wget \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\" -O data.zip\n",
        "!unzip -o data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5a710f7a-74b4-48c3-98d1-a6d409a7af1a",
      "metadata": {
        "id": "5a710f7a-74b4-48c3-98d1-a6d409a7af1a",
        "outputId": "fb840754-8360-459d-fe4b-bbe92b24475e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing file: /home/theebatican/Documents/CpE_AY23-24_S2/itanong/csv_data/Categories.csv\n",
            "processing file: /home/theebatican/Documents/CpE_AY23-24_S2/itanong/csv_data/Products.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"/home/theebatican/Documents/CpE_AY23-24_S2/itanong/csv_data\")\n",
        "csv_files = sorted([f for f in data_dir.glob(\"*.csv\")])\n",
        "dfs = []\n",
        "for csv_file in csv_files:\n",
        "    print(f\"processing file: {csv_file}\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {csv_file}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c271b986-9ed6-4a8a-a7c2-1dad7a642c06",
      "metadata": {
        "id": "c271b986-9ed6-4a8a-a7c2-1dad7a642c06"
      },
      "source": [
        "### Extract Table Name and Summary from each Table\n",
        "\n",
        "Here we use gpt-3.5 to extract a table name (with underscores) and summary from each table with our Pydantic program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4cef9585-87f8-4427-aaca-c0038c289e00",
      "metadata": {
        "id": "4cef9585-87f8-4427-aaca-c0038c289e00",
        "outputId": "03b3f4e4-93cf-4125-b745-a5f190208bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘BOC_VegFruits_tableinfo’: File exists\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "tableinfo_dir = \"BOC_VegFruits_tableinfo\"\n",
        "!mkdir {tableinfo_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9a81afbf-0870-41b1-baea-017026a1b7d6",
      "metadata": {
        "id": "9a81afbf-0870-41b1-baea-017026a1b7d6"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class TableInfo(BaseModel):\n",
        "    \"\"\"Information regarding a structured table.\"\"\"\n",
        "\n",
        "    table_name: str = Field(\n",
        "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
        "    )\n",
        "    table_summary: str = Field(\n",
        "        ..., description=\"short, concise summary/caption of the table\"\n",
        "    )\n",
        "\n",
        "\n",
        "prompt_str = \"\"\"\\\n",
        "Give me a summary of the table with the following JSON format.\n",
        "\n",
        "- The table name must be unique to the table and describe it while being concise.\n",
        "- Do NOT output a generic table name (e.g. table, my_table).\n",
        "\n",
        "Do NOT make the table name one of the following: {exclude_table_name_list}\n",
        "\n",
        "Table:\n",
        "{table_str}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "program = LLMTextCompletionProgram.from_defaults(\n",
        "    output_cls=TableInfo,\n",
        "    llm=locally_run,\n",
        "    prompt_template_str=prompt_str,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f50a3c04-d1fb-4964-8f37-741fb98a5249",
      "metadata": {
        "id": "f50a3c04-d1fb-4964-8f37-741fb98a5249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def _get_tableinfo_with_index(idx: int) -> str:\n",
        "    results_gen = Path(tableinfo_dir).glob(f\"{idx}_*\")\n",
        "    results_list = list(results_gen)\n",
        "    if len(results_list) == 0:\n",
        "        return None\n",
        "    elif len(results_list) == 1:\n",
        "        path = results_list[0]\n",
        "        return TableInfo.parse_file(path)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"More than one file matching index: {list(results_gen)}\"\n",
        "        )\n",
        "\n",
        "\n",
        "table_names = set()\n",
        "table_infos = []\n",
        "for idx, df in enumerate(dfs):\n",
        "    table_info = _get_tableinfo_with_index(idx)\n",
        "    if table_info:\n",
        "        table_infos.append(table_info)\n",
        "    else:\n",
        "        while True:\n",
        "            df_str = df.head(10).to_csv()\n",
        "            table_info = program(\n",
        "                table_str=df_str,\n",
        "                exclude_table_name_list=str(list(table_names)),\n",
        "            )\n",
        "            table_name = table_info.table_name\n",
        "            print(f\"Processed table: {table_name}\")\n",
        "            if table_name not in table_names:\n",
        "                table_names.add(table_name)\n",
        "                break\n",
        "            else:\n",
        "                # try again\n",
        "                print(f\"Table name {table_name} already exists, trying again.\")\n",
        "                pass\n",
        "\n",
        "        out_file = f\"{tableinfo_dir}/{idx}_{table_name}.json\"\n",
        "        json.dump(table_info.dict(), open(out_file, \"w\"))\n",
        "    table_infos.append(table_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1cc3230-d14c-4491-b79f-45159708badb",
      "metadata": {
        "id": "a1cc3230-d14c-4491-b79f-45159708badb"
      },
      "source": [
        "### Put Data in SQL Database\n",
        "\n",
        "We use `sqlalchemy`, a popular SQL database toolkit, to load all the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53544059-de7d-48dd-8e00-89517964852b",
      "metadata": {
        "id": "53544059-de7d-48dd-8e00-89517964852b",
        "outputId": "df224651-1765-4aa7-e841-d58546c20e84"
      },
      "outputs": [],
      "source": [
        "# put data into sqlite db\n",
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        ")\n",
        "import re\n",
        "\n",
        "\n",
        "# Function to create a sanitized column name\n",
        "def sanitize_column_name(col_name):\n",
        "    # Remove special characters and replace spaces with underscores\n",
        "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
        "\n",
        "\n",
        "# Function to create a table from a DataFrame using SQLAlchemy\n",
        "def create_table_from_dataframe(\n",
        "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
        "):\n",
        "    # Sanitize column names\n",
        "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
        "    df = df.rename(columns=sanitized_columns)\n",
        "\n",
        "    # Dynamically create columns based on DataFrame columns and data types\n",
        "    columns = [\n",
        "        Column(col, String if dtype == \"object\" else Integer)\n",
        "        for col, dtype in zip(df.columns, df.dtypes)\n",
        "    ]\n",
        "\n",
        "    # Create a table with the defined columns\n",
        "    table = Table(table_name, metadata_obj, *columns)\n",
        "\n",
        "    # Create the table in the database\n",
        "    metadata_obj.create_all(engine)\n",
        "\n",
        "    # Insert data from DataFrame into the table\n",
        "    with engine.connect() as conn:\n",
        "        for _, row in df.iterrows():\n",
        "            insert_stmt = table.insert().values(**row.to_dict())\n",
        "            conn.execute(insert_stmt)\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\")\n",
        "metadata_obj = MetaData()\n",
        "for idx, df in enumerate(dfs):\n",
        "    tableinfo = _get_tableinfo_with_index(idx)\n",
        "    print(f\"Creating table: {tableinfo.table_name}\")\n",
        "    create_table_from_dataframe(df, tableinfo.table_name, engine, metadata_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
      "metadata": {
        "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
        "outputId": "4527a7d1-6b5f-4f89-9a63-e5263f4e4441"
      },
      "outputs": [],
      "source": [
        "# setup Arize Phoenix for logging/observability\n",
        "import phoenix as px\n",
        "import llama_index\n",
        "\n",
        "px.launch_app()\n",
        "llama_index.set_global_handler(\"arize_phoenix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5906e7-4e20-4b0a-bc42-615f6b86beb7",
      "metadata": {
        "id": "1b5906e7-4e20-4b0a-bc42-615f6b86beb7"
      },
      "source": [
        "## Advanced Capability 1: Text-to-SQL with Query-Time Table Retrieval.\n",
        "\n",
        "We now show you how to setup an e2e text-to-SQL with table retrieval.\n",
        "\n",
        "### Define Modules\n",
        "\n",
        "Here we define the core modules.\n",
        "1. Object index + retriever to store table schemas\n",
        "2. SQLDatabase object to connect to the above tables + SQLRetriever.\n",
        "3. Text-to-SQL Prompt\n",
        "4. Response synthesis Prompt\n",
        "5. LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0830ecca-9688-45b7-b011-d6d44d6fc551",
      "metadata": {
        "id": "0830ecca-9688-45b7-b011-d6d44d6fc551"
      },
      "source": [
        "Object index, retriever, SQLDatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019",
      "metadata": {
        "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019"
      },
      "outputs": [],
      "source": [
        "from llama_index.objects import (\n",
        "    SQLTableNodeMapping,\n",
        "    ObjectIndex,\n",
        "    SQLTableSchema,\n",
        ")\n",
        "from llama_index import SQLDatabase, VectorStoreIndex\n",
        "\n",
        "sql_database = SQLDatabase(engine)\n",
        "\n",
        "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
        "table_schema_objs = [\n",
        "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
        "    for t in table_infos\n",
        "]  # add a SQLTableSchema for each table\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    table_schema_objs,\n",
        "    table_node_mapping,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4620fbe4-eac6-4476-b373-0e48dfbd81e0",
      "metadata": {
        "id": "4620fbe4-eac6-4476-b373-0e48dfbd81e0"
      },
      "source": [
        "SQLRetriever + Table Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9d68bb-16a5-409c-a406-1432565dde99",
      "metadata": {
        "id": "3e9d68bb-16a5-409c-a406-1432565dde99"
      },
      "outputs": [],
      "source": [
        "from llama_index.retrievers import SQLRetriever\n",
        "from typing import List\n",
        "from llama_index.query_pipeline import FnComponent\n",
        "\n",
        "sql_retriever = SQLRetriever(sql_database)\n",
        "\n",
        "\n",
        "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
        "    \"\"\"Get table context string.\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        table_info = sql_database.get_single_table_info(\n",
        "            table_schema_obj.table_name\n",
        "        )\n",
        "        if table_schema_obj.context_str:\n",
        "            table_opt_context = \" The table description is: \"\n",
        "            table_opt_context += table_schema_obj.context_str\n",
        "            table_info += table_opt_context\n",
        "\n",
        "        context_strs.append(table_info)\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "\n",
        "table_parser_component = FnComponent(fn=get_table_context_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830ff35b-72ba-42bc-ab49-14b2efc93d17",
      "metadata": {
        "id": "830ff35b-72ba-42bc-ab49-14b2efc93d17"
      },
      "source": [
        "Text-to-SQL Prompt + Output Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246b8a28-a5b8-4103-b731-a6c09f8694ed",
      "metadata": {
        "id": "246b8a28-a5b8-4103-b731-a6c09f8694ed",
        "outputId": "479f47f2-f530-4984-d0fe-ded7a02a306e"
      },
      "outputs": [],
      "source": [
        "from llama_index.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
        "from llama_index.prompts import PromptTemplate\n",
        "from llama_index.query_pipeline import FnComponent\n",
        "from llama_index.llms import ChatResponse\n",
        "\n",
        "\n",
        "def parse_response_to_sql(response: ChatResponse) -> str:\n",
        "    \"\"\"Parse response to SQL.\"\"\"\n",
        "    response = response.message.content\n",
        "    sql_query_start = response.find(\"SQLQuery:\")\n",
        "    if sql_query_start != -1:\n",
        "        response = response[sql_query_start:]\n",
        "        # TODO: move to removeprefix after Python 3.9+\n",
        "        if response.startswith(\"SQLQuery:\"):\n",
        "            response = response[len(\"SQLQuery:\") :]\n",
        "    sql_result_start = response.find(\"SQLResult:\")\n",
        "    if sql_result_start != -1:\n",
        "        response = response[:sql_result_start]\n",
        "    return response.strip().strip(\"```\").strip()\n",
        "\n",
        "\n",
        "sql_parser_component = FnComponent(fn=parse_response_to_sql)\n",
        "\n",
        "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
        "    dialect=engine.dialect.name\n",
        ")\n",
        "print(text2sql_prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cf6a211-dd6b-4ffc-b781-0b0c38896e25",
      "metadata": {
        "id": "0cf6a211-dd6b-4ffc-b781-0b0c38896e25"
      },
      "source": [
        "Response Synthesis Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fe48e8-ffdd-48f8-b59a-d13d973e8a19",
      "metadata": {
        "id": "a2fe48e8-ffdd-48f8-b59a-d13d973e8a19"
      },
      "outputs": [],
      "source": [
        "response_synthesis_prompt_str = (\n",
        "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"SQL: {sql_query}\\n\"\n",
        "    \"SQL Response: {context_str}\\n\"\n",
        "    \"Response: \"\n",
        ")\n",
        "response_synthesis_prompt = PromptTemplate(\n",
        "    response_synthesis_prompt_str,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0d0a62-beeb-4146-99ee-fbe3875481db",
      "metadata": {
        "id": "dc0d0a62-beeb-4146-99ee-fbe3875481db"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ad7cc4-763b-4060-8cc3-ac198c6b956c",
      "metadata": {
        "id": "d4ad7cc4-763b-4060-8cc3-ac198c6b956c"
      },
      "source": [
        "### Define Query Pipeline\n",
        "\n",
        "Now that the components are in place, let's define the query pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d46d945-0efe-4e7f-b2a8-3ea56896f0be",
      "metadata": {
        "id": "9d46d945-0efe-4e7f-b2a8-3ea56896f0be"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_pipeline import (\n",
        "    QueryPipeline as QP,\n",
        "    Link,\n",
        "    InputComponent,\n",
        "    CustomQueryComponent,\n",
        ")\n",
        "\n",
        "qp = QP(\n",
        "    modules={\n",
        "        \"input\": InputComponent(),\n",
        "        \"table_retriever\": obj_retriever,\n",
        "        \"table_output_parser\": table_parser_component,\n",
        "        \"text2sql_prompt\": text2sql_prompt,\n",
        "        \"text2sql_llm\": llm,\n",
        "        \"sql_output_parser\": sql_parser_component,\n",
        "        \"sql_retriever\": sql_retriever,\n",
        "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "        \"response_synthesis_llm\": llm,\n",
        "    },\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca77592-872e-4751-906d-25c396b3d49f",
      "metadata": {
        "id": "eca77592-872e-4751-906d-25c396b3d49f"
      },
      "outputs": [],
      "source": [
        "qp.add_chain([\"input\", \"table_retriever\", \"table_output_parser\"])\n",
        "qp.add_link(\"input\", \"text2sql_prompt\", dest_key=\"query_str\")\n",
        "qp.add_link(\"table_output_parser\", \"text2sql_prompt\", dest_key=\"schema\")\n",
        "qp.add_chain(\n",
        "    [\"text2sql_prompt\", \"text2sql_llm\", \"sql_output_parser\", \"sql_retriever\"]\n",
        ")\n",
        "qp.add_link(\n",
        "    \"sql_output_parser\", \"response_synthesis_prompt\", dest_key=\"sql_query\"\n",
        ")\n",
        "qp.add_link(\n",
        "    \"sql_retriever\", \"response_synthesis_prompt\", dest_key=\"context_str\"\n",
        ")\n",
        "qp.add_link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\")\n",
        "qp.add_link(\"response_synthesis_prompt\", \"response_synthesis_llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e250a417-42c3-49c2-a037-a282974d5f9a",
      "metadata": {
        "id": "e250a417-42c3-49c2-a037-a282974d5f9a"
      },
      "source": [
        "### Visualize Query Pipeline\n",
        "\n",
        "A really nice property of the query pipeline syntax is you can easily visualize it in a graph via networkx."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5f419c-c128-4b04-b18f-0f8b671ff3a4",
      "metadata": {
        "id": "8c5f419c-c128-4b04-b18f-0f8b671ff3a4",
        "outputId": "713e82d3-ddae-4a41-bd65-0cfd802eae9f"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
        "net.from_nx(qp.dag)\n",
        "net.show(\"text2sql_dag.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b525ef9b-cb94-401e-8fd8-47c94eb3eaa5",
      "metadata": {
        "id": "b525ef9b-cb94-401e-8fd8-47c94eb3eaa5"
      },
      "source": [
        "### Run Some Queries!\n",
        "\n",
        "Now we're ready to run some queries across this entire pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a9231a-54ba-465d-a024-acf94a5baf90",
      "metadata": {
        "id": "39a9231a-54ba-465d-a024-acf94a5baf90",
        "outputId": "67c08464-fbdf-4e00-99ef-f5850965dd81"
      },
      "outputs": [],
      "source": [
        "response = qp.run(\n",
        "    query=\"What was the year that The Notorious B.I.G was signed to Bad Boy?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70033ca-8d46-4ac5-a2e3-869e5abad0af",
      "metadata": {
        "id": "b70033ca-8d46-4ac5-a2e3-869e5abad0af"
      },
      "outputs": [],
      "source": [
        "response = qp.run(query=\"Who won best director in the 1972 academy awards\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dcfdd6-5a1f-4169-b480-5fc75a9d137e",
      "metadata": {
        "id": "23dcfdd6-5a1f-4169-b480-5fc75a9d137e"
      },
      "outputs": [],
      "source": [
        "response = qp.run(query=\"What was the term of Pasquale Preziosa?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fb1122-a15f-4b13-b8e0-6dd23e4fb893",
      "metadata": {
        "id": "a0fb1122-a15f-4b13-b8e0-6dd23e4fb893"
      },
      "source": [
        "## 2. Advanced Capability 2: Text-to-SQL with Query-Time Row Retrieval (along with Table Retrieval)\n",
        "\n",
        "One problem in the previous example is that if the user asks a query that asks for \"The Notorious BIG\" but the artist is stored as \"The Notorious B.I.G\", then the generated SELECT statement will likely not return any matches.\n",
        "\n",
        "We can alleviate this problem by fetching a small number of example rows per table. A naive option would be to just take the first k rows. Instead, we embed, index, and retrieve k relevant rows given the user query to give the text-to-SQL LLM the most contextually relevant information for SQL generation.\n",
        "\n",
        "We now extend our query pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5984f5db-7bfe-41ff-9cfe-2cde0e0a5d94",
      "metadata": {
        "id": "5984f5db-7bfe-41ff-9cfe-2cde0e0a5d94"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_pipeline import QueryPipeline as QP\n",
        "from llama_index.service_context import ServiceContext\n",
        "\n",
        "qp = QP(verbose=True)\n",
        "# NOTE: service context will be deprecated in v0.10 (though will still be backwards compatible)\n",
        "service_context = ServiceContext.from_defaults(callback_manager=qp.callback_manager)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edee5340-b4f9-4594-b62d-376d1ef8cf75",
      "metadata": {
        "id": "edee5340-b4f9-4594-b62d-376d1ef8cf75"
      },
      "source": [
        "### Index Each Table\n",
        "\n",
        "We embed/index the rows of each table, resulting in one index per table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c0fcb2-b079-48f2-8c54-bf5c66940173",
      "metadata": {
        "id": "a5c0fcb2-b079-48f2-8c54-bf5c66940173",
        "outputId": "e8eefac0-5d06-46c5-89af-0de57363db2e"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, load_index_from_storage\n",
        "from sqlalchemy import text\n",
        "from llama_index.schema import TextNode\n",
        "from llama_index.storage import StorageContext\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def index_all_tables(\n",
        "    sql_database: SQLDatabase, table_index_dir: str = \"table_index_dir\"\n",
        ") -> Dict[str, VectorStoreIndex]:\n",
        "    \"\"\"Index all tables.\"\"\"\n",
        "    if not Path(table_index_dir).exists():\n",
        "        os.makedirs(table_index_dir)\n",
        "\n",
        "    vector_index_dict = {}\n",
        "    engine = sql_database.engine\n",
        "    for table_name in sql_database.get_usable_table_names():\n",
        "        print(f\"Indexing rows in table: {table_name}\")\n",
        "        if not os.path.exists(f\"{table_index_dir}/{table_name}\"):\n",
        "            # get all rows from table\n",
        "            with engine.connect() as conn:\n",
        "                cursor = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
        "                result = cursor.fetchall()\n",
        "                row_tups = []\n",
        "                for row in result:\n",
        "                    row_tups.append(tuple(row))\n",
        "\n",
        "            # index each row, put into vector store index\n",
        "            nodes = [TextNode(text=str(t)) for t in row_tups]\n",
        "\n",
        "            # put into vector store index (use OpenAIEmbeddings by default)\n",
        "            index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "\n",
        "            # save index\n",
        "            index.set_index_id(\"vector_index\")\n",
        "            index.storage_context.persist(f\"{table_index_dir}/{table_name}\")\n",
        "        else:\n",
        "            # rebuild storage context\n",
        "            storage_context = StorageContext.from_defaults(\n",
        "                persist_dir=f\"{table_index_dir}/{table_name}\"\n",
        "            )\n",
        "            # load index\n",
        "            index = load_index_from_storage(\n",
        "                storage_context, index_id=\"vector_index\", service_context=service_context\n",
        "            )\n",
        "        vector_index_dict[table_name] = index\n",
        "\n",
        "    return vector_index_dict\n",
        "\n",
        "\n",
        "vector_index_dict = index_all_tables(sql_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06a001a-7e9b-41e4-94f3-019d3009fe6a",
      "metadata": {
        "id": "f06a001a-7e9b-41e4-94f3-019d3009fe6a",
        "outputId": "579cd0b2-cb2a-4b8d-82c2-b91a425b9069"
      },
      "outputs": [],
      "source": [
        "test_retriever = vector_index_dict[\"Bad_Boy_Artists\"].as_retriever(\n",
        "    similarity_top_k=1\n",
        ")\n",
        "nodes = test_retriever.retrieve(\"P. Diddy\")\n",
        "print(nodes[0].get_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f598a0a-1a3d-42a7-8640-fc1309d8b387",
      "metadata": {
        "id": "5f598a0a-1a3d-42a7-8640-fc1309d8b387"
      },
      "source": [
        "### Define Expanded Table Parser Component\n",
        "\n",
        "We expand the capability of our `table_parser_component` to not only return the relevant table schemas, but also return relevant rows per table schema.\n",
        "\n",
        "It now takes in both `table_schema_objs` (output of table retriever), but also the original `query_str` which will then be used for vector retrieval of relevant rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22340e9c-382c-4b7d-9266-d53a72af3023",
      "metadata": {
        "id": "22340e9c-382c-4b7d-9266-d53a72af3023"
      },
      "outputs": [],
      "source": [
        "from llama_index.retrievers import SQLRetriever\n",
        "from typing import List\n",
        "from llama_index.query_pipeline import FnComponent\n",
        "\n",
        "sql_retriever = SQLRetriever(sql_database)\n",
        "\n",
        "\n",
        "def get_table_context_and_rows_str(\n",
        "    query_str: str, table_schema_objs: List[SQLTableSchema]\n",
        "):\n",
        "    \"\"\"Get table context string.\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        # first append table info + additional context\n",
        "        table_info = sql_database.get_single_table_info(\n",
        "            table_schema_obj.table_name\n",
        "        )\n",
        "        if table_schema_obj.context_str:\n",
        "            table_opt_context = \" The table description is: \"\n",
        "            table_opt_context += table_schema_obj.context_str\n",
        "            table_info += table_opt_context\n",
        "\n",
        "        # also lookup vector index to return relevant table rows\n",
        "        vector_retriever = vector_index_dict[\n",
        "            table_schema_obj.table_name\n",
        "        ].as_retriever(similarity_top_k=2)\n",
        "        relevant_nodes = vector_retriever.retrieve(query_str)\n",
        "        if len(relevant_nodes) > 0:\n",
        "            table_row_context = \"\\nHere are some relevant example rows (values in the same order as columns above)\\n\"\n",
        "            for node in relevant_nodes:\n",
        "                table_row_context += str(node.get_content()) + \"\\n\"\n",
        "            table_info += table_row_context\n",
        "\n",
        "        context_strs.append(table_info)\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "\n",
        "table_parser_component = FnComponent(fn=get_table_context_and_rows_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff926459-8161-4405-a2e8-3b074e925748",
      "metadata": {
        "id": "ff926459-8161-4405-a2e8-3b074e925748"
      },
      "source": [
        "### Define Expanded Query Pipeline\n",
        "\n",
        "This looks similar to the query pipeline in section 1, but with an upgraded table_parser_component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d654a0-47bf-47c4-b101-97c55463f5fd",
      "metadata": {
        "id": "54d654a0-47bf-47c4-b101-97c55463f5fd"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_pipeline import (\n",
        "    QueryPipeline as QP,\n",
        "    Link,\n",
        "    InputComponent,\n",
        "    CustomQueryComponent,\n",
        ")\n",
        "\n",
        "qp.add_modules({\n",
        "    \"input\": InputComponent(),\n",
        "    \"table_retriever\": obj_retriever,\n",
        "    \"table_output_parser\": table_parser_component,\n",
        "    \"text2sql_prompt\": text2sql_prompt,\n",
        "    \"text2sql_llm\": llm,\n",
        "    \"sql_output_parser\": sql_parser_component,\n",
        "    \"sql_retriever\": sql_retriever,\n",
        "    \"response_synthesis_prompt\": response_synthesis_prompt,\n",
        "    \"response_synthesis_llm\": llm,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b921b6-b8da-4901-b1a7-a21df6f1ccb1",
      "metadata": {
        "id": "b0b921b6-b8da-4901-b1a7-a21df6f1ccb1"
      },
      "outputs": [],
      "source": [
        "qp.add_link(\"input\", \"table_retriever\")\n",
        "qp.add_link(\"input\", \"table_output_parser\", dest_key=\"query_str\")\n",
        "qp.add_link(\n",
        "    \"table_retriever\", \"table_output_parser\", dest_key=\"table_schema_objs\"\n",
        ")\n",
        "qp.add_link(\"input\", \"text2sql_prompt\", dest_key=\"query_str\")\n",
        "qp.add_link(\"table_output_parser\", \"text2sql_prompt\", dest_key=\"schema\")\n",
        "qp.add_chain(\n",
        "    [\"text2sql_prompt\", \"text2sql_llm\", \"sql_output_parser\", \"sql_retriever\"]\n",
        ")\n",
        "qp.add_link(\n",
        "    \"sql_output_parser\", \"response_synthesis_prompt\", dest_key=\"sql_query\"\n",
        ")\n",
        "qp.add_link(\n",
        "    \"sql_retriever\", \"response_synthesis_prompt\", dest_key=\"context_str\"\n",
        ")\n",
        "qp.add_link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\")\n",
        "qp.add_link(\"response_synthesis_prompt\", \"response_synthesis_llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a2f78d4-d2cf-460f-a218-c47b7a98a67e",
      "metadata": {
        "id": "8a2f78d4-d2cf-460f-a218-c47b7a98a67e",
        "outputId": "6759b690-54e5-4b83-eaed-0ae429669c48"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
        "net.from_nx(qp.dag)\n",
        "net.show(\"text2sql_dag.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16400cfc-7a21-45fc-932d-b917ab97babe",
      "metadata": {
        "id": "16400cfc-7a21-45fc-932d-b917ab97babe"
      },
      "source": [
        "### Run Some Queries\n",
        "\n",
        "We can now ask about relevant entries even if it doesn't exactly match the entry in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54c61a4-87f2-401d-8a3f-955105303883",
      "metadata": {
        "id": "c54c61a4-87f2-401d-8a3f-955105303883",
        "outputId": "8d73da17-42ba-4f53-cc06-89e7118a18e4"
      },
      "outputs": [],
      "source": [
        "response = qp.run(\n",
        "    query=\"What was the year that The Notorious BIG was signed to Bad Boy?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4821ef4a-aeff-4082-ae2c-accef0068c40",
      "metadata": {
        "id": "4821ef4a-aeff-4082-ae2c-accef0068c40"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "itanong",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
